"""Evaluator adapter — direct evaluator invocation via Strands Evals SDK.

Wraps strands_evals evaluators for direct invocation on pre-existing
input/output without requiring an agent. Uses the SDK's EvaluationData
and EvaluationOutput types exactly as documented.

Strands SDK pattern:
    evaluator = OutputEvaluator(rubric="...")
    result = evaluator.evaluate(EvaluationData(
        input=case.input, actual_output=agent_output
    ))
    # result -> list[EvaluationOutput] with score, test_pass, reason, label

See: https://strandsagents.com/latest/documentation/docs/user-guide/evals-sdk/evaluators/
"""
from __future__ import annotations

import logging
from typing import Any

logger = logging.getLogger(__name__)


def evaluate_output(
    input_text: str,
    output_text: str,
    evaluator_name: str = "output",
    rubric: str = "",
    expected_output: str | None = None,
) -> dict[str, Any]:
    """Run a single Strands evaluator on pre-existing input/output.

    Directly invokes evaluator.evaluate(EvaluationData(...)) —
    no agent is spun up. Use this to evaluate text that was
    already generated by an LLM or any other source.

    Args:
        input_text: The original input/query/prompt.
        output_text: The output to evaluate.
        evaluator_name: Evaluator to use. Options: output,
            helpfulness, faithfulness, coherence, conciseness,
            harmfulness, response_relevance, tool_selection,
            tool_parameter, trajectory, interactions, goal_success.
        rubric: Custom rubric (required for output/trajectory).
        expected_output: Optional expected output for comparison.

    Returns:
        Evaluation result with score (0.0-1.0), test_pass,
        reason, and label per Strands EvaluationOutput.
    """
    from strands_evals.types import EvaluationData
    from .evaluator_factory import build_evaluators

    evaluators = build_evaluators([evaluator_name], rubric)
    evaluator = evaluators[0]

    kwargs: dict[str, Any] = {
        "input": input_text,
        "actual_output": output_text,
    }
    if expected_output is not None:
        kwargs["expected_output"] = expected_output

    eval_data = EvaluationData(**kwargs)
    results = evaluator.evaluate(eval_data)

    if not results:
        return {
            "score": 0.0, "test_pass": False,
            "reason": "No evaluation output", "label": "",
            "evaluator": evaluator_name,
        }

    r = results[0]
    return {
        "score": r.score,
        "test_pass": r.test_pass,
        "reason": r.reason,
        "label": getattr(r, "label", ""),
        "evaluator": evaluator_name,
    }


def evaluate_output_multi(
    input_text: str,
    output_text: str,
    evaluator_names: list[str],
    rubric: str = "",
    expected_output: str | None = None,
) -> dict[str, Any]:
    """Run multiple Strands evaluators on the same input/output.

    Follows the SDK's "Combine Multiple Evaluators" pattern.
    Each evaluator scores independently; results are aggregated.

    Args:
        input_text: The original input/query/prompt.
        output_text: The output to evaluate.
        evaluator_names: List of evaluator short names.
        rubric: Custom rubric for evaluators that need one.
        expected_output: Optional expected output for comparison.

    Returns:
        Per-evaluator results and aggregate summary with
        avg_score and pass_rate.
    """
    from strands_evals.types import EvaluationData
    from .evaluator_factory import build_evaluators

    evaluators = build_evaluators(evaluator_names, rubric)

    kwargs: dict[str, Any] = {
        "input": input_text,
        "actual_output": output_text,
    }
    if expected_output is not None:
        kwargs["expected_output"] = expected_output

    eval_data = EvaluationData(**kwargs)

    results = []
    for evaluator, name in zip(evaluators, evaluator_names):
        try:
            eval_results = evaluator.evaluate(eval_data)
            r = eval_results[0] if eval_results else None
            if r:
                results.append({
                    "evaluator": name,
                    "score": r.score,
                    "test_pass": r.test_pass,
                    "reason": r.reason,
                    "label": getattr(r, "label", ""),
                })
            else:
                results.append({
                    "evaluator": name, "score": 0.0,
                    "test_pass": False, "reason": "No output",
                    "label": "",
                })
        except Exception as e:
            logger.warning("Evaluator %s failed: %s", name, e)
            results.append({
                "evaluator": name, "score": 0.0,
                "test_pass": False, "reason": str(e),
                "label": "error",
            })

    scores = [r["score"] for r in results if r.get("label") != "error"]
    passes = [r["test_pass"] for r in results if r.get("label") != "error"]
    return {
        "results": results,
        "summary": {
            "avg_score": sum(scores) / len(scores) if scores else 0.0,
            "pass_rate": (
                sum(1 for p in passes if p) / len(passes)
                if passes else 0.0
            ),
            "total_evaluators": len(results),
        },
    }

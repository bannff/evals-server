"""Strands Evals SDK experiment orchestration.

Converts EvalCase/ExperimentConfig to Strands SDK types, runs
experiments via Experiment.run_evaluations, and generates
experiment configs from context.
"""
from __future__ import annotations

import asyncio
import logging
from typing import Any

from ..ports import EvalCase, ExperimentConfig, ExperimentReport
from .evaluator_factory import build_evaluators
from .agent_task import build_agent_fn

logger = logging.getLogger(__name__)


def _our_cases_to_strands(cases: list[EvalCase]) -> list[Any]:
    """Convert our EvalCase list to Strands Case list."""
    from strands_evals import Case

    strands_cases = []
    for c in cases:
        raw = c.input
        input_str = raw if isinstance(raw, str) else raw.get("query", raw.get("input", str(raw)))
        expected = None
        if c.expected:
            expected = c.expected.get(
                "output", c.expected.get("response", str(c.expected)),
            )
        kwargs: dict[str, Any] = {
            "name": c.name or c.id,
            "input": input_str,
            "metadata": c.metadata,
        }
        if expected is not None:
            kwargs["expected_output"] = expected
        if c.expected_trajectory:
            kwargs["expected_trajectory"] = c.expected_trajectory
        strands_cases.append(Case[str, str](**kwargs))
    return strands_cases


def run_strands_experiment(config: ExperimentConfig) -> ExperimentReport:
    """Run a Strands experiment end-to-end."""
    from strands_evals import Experiment

    strands_cases = _our_cases_to_strands(config.cases)
    evaluators = build_evaluators(config.evaluator_names, config.rubric)
    task_fn = build_agent_fn(config.agent_config, config.evaluator_names)

    experiment = Experiment[str, str](
        cases=strands_cases, evaluators=evaluators,
    )
    reports = experiment.run_evaluations(task_fn)
    report = reports[0] if reports else None

    case_results: list[dict[str, Any]] = []
    summary: dict[str, Any] = {}
    if report:
        for i, case_rec in enumerate(report.cases):
            case_results.append({
                "case_name": case_rec.get("name", f"case-{i}"),
                "score": report.scores[i] if i < len(report.scores) else 0.0,
                "passed": report.test_passes[i] if i < len(report.test_passes) else False,
                "reason": report.reasons[i] if i < len(report.reasons) else "",
            })
        passes = report.test_passes
        summary = {
            "overall_score": report.overall_score,
            "pass_rate": sum(passes) / len(passes) if passes else 0.0,
            "total_cases": len(report.cases),
        }

    return ExperimentReport(
        experiment_name=config.name or "experiment",
        case_results=case_results,
        summary=summary,
        evaluator_names=config.evaluator_names,
    )


def generate_strands_experiment(
    context: str,
    task_description: str,
    num_cases: int = 5,
    evaluator_name: str = "output",
) -> ExperimentConfig:
    """Generate an experiment config from context using ExperimentGenerator."""
    from strands_evals.generators import ExperimentGenerator
    from strands_evals.evaluators import OutputEvaluator, TrajectoryEvaluator

    eval_map = {"output": OutputEvaluator, "trajectory": TrajectoryEvaluator}
    evaluator_cls = eval_map.get(evaluator_name, OutputEvaluator)

    generator = ExperimentGenerator[str, str](
        input_type=str, output_type=str, include_expected_output=True,
    )

    async def _generate():
        return await generator.from_context_async(
            context=context, task_description=task_description,
            num_cases=num_cases, evaluator=evaluator_cls,
        )

    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None

    if loop and loop.is_running():
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor() as pool:
            experiment = pool.submit(asyncio.run, _generate()).result()
    else:
        experiment = asyncio.run(_generate())

    cases = []
    for i, sc in enumerate(experiment.cases):
        cases.append(EvalCase(
            id=f"gen-{i}",
            name=sc.name or f"case-{i}",
            input={"query": sc.input} if isinstance(sc.input, str) else sc.input,
            expected={"output": sc.expected_output} if sc.expected_output else None,
            metadata=sc.metadata or {},
        ))

    return ExperimentConfig(
        cases=cases,
        evaluator_names=[evaluator_name],
        name=f"generated-{task_description[:30]}",
    )

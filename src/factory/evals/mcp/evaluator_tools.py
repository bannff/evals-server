"""MCP tools for direct Strands evaluator invocation.

Exposes evaluator.evaluate(EvaluationData(...)) through MCP —
no agent needed. Evaluate pre-existing input/output directly.

See: https://strandsagents.com/latest/documentation/docs/user-guide/evals-sdk/evaluators/
"""
from __future__ import annotations

from typing import Any, Callable, TYPE_CHECKING

from fastmcp import FastMCP
from factory.mcp_utils.interface import operational

if TYPE_CHECKING:
    from ..runtime.runtime import EvalsRuntime


def register(
    mcp: FastMCP,
    get_runtime: Callable[[], "EvalsRuntime"],
) -> None:
    """Register evaluator tools."""

    @mcp.tool()
    @operational
    def evals_evaluate(
        input_text: str,
        output_text: str,
        evaluator_name: str = "output",
        rubric: str = "",
        expected_output: str | None = None,
    ) -> dict[str, Any]:
        """Run a Strands evaluator on pre-existing input/output.

        Directly invokes evaluator.evaluate(EvaluationData(...)) —
        no agent is spun up. Use this to evaluate text that was
        already generated by an LLM or any other source.

        Args:
            input_text: The original input/query/prompt.
            output_text: The output to evaluate.
            evaluator_name: Evaluator to use. Options: output,
                helpfulness, faithfulness, coherence, conciseness,
                harmfulness, response_relevance, tool_selection,
                tool_parameter, trajectory, interactions, goal_success.
            rubric: Custom rubric (required for output/trajectory).
            expected_output: Optional expected output for comparison.

        Returns:
            Evaluation result with score (0.0-1.0), test_pass,
            reason, and label per Strands EvaluationOutput.
        """
        runtime = get_runtime()
        try:
            return runtime.evaluate_output(
                input_text, output_text, evaluator_name,
                rubric, expected_output,
            )
        except RuntimeError as e:
            return {"error": str(e)}

    @mcp.tool()
    @operational
    def evals_evaluate_multi(
        input_text: str,
        output_text: str,
        evaluator_names: list[str],
        rubric: str = "",
        expected_output: str | None = None,
    ) -> dict[str, Any]:
        """Run multiple Strands evaluators on the same input/output.

        Follows the SDK's "Combine Multiple Evaluators" pattern.
        Each evaluator scores independently; results are aggregated.

        Args:
            input_text: The original input/query/prompt.
            output_text: The output to evaluate.
            evaluator_names: List of evaluator names to run.
            rubric: Custom rubric for evaluators that need one.
            expected_output: Optional expected output for comparison.

        Returns:
            Per-evaluator results and aggregate summary with
            avg_score and pass_rate.
        """
        runtime = get_runtime()
        try:
            return runtime.evaluate_output_multi(
                input_text, output_text, evaluator_names,
                rubric, expected_output,
            )
        except RuntimeError as e:
            return {"error": str(e)}
